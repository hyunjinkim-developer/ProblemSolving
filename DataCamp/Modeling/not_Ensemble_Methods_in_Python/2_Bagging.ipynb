{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## The strength of “weak” models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Restricted and unrestricted decision trees\n",
    "\n",
    "# Build unrestricted decision tree\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "#  Confusion matrix:\n",
    "#      [[143   7]\n",
    "#      [  3   7]]\n",
    "\n",
    "# Print the F1 score\n",
    "score = f1_score(y_test, pred)\n",
    "print('F1-Score: {:.3f}'.format(score))\n",
    "#     F1-Score: 0.583\n",
    "\n",
    "\n",
    "# Build restricted decision tree\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=3, min_samples_split=9, random_state=500, max_depth=4, max_features=2)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Print the confusion matrix\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "print('Confusion matrix:\\n', cm)\n",
    "# Confusion matrix:\n",
    "#      [[146   4]\n",
    "#      [  5   5]]\n",
    "\n",
    "# Print the F1 score\n",
    "score = f1_score(y_test, pred)\n",
    "print('F1-Score: {:.3f}'.format(score))\n",
    "#     F1-Score: 0.526"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bootstrap aggregating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Training with bootstrapping\n",
    "# Take a sample with replacement\n",
    "X_train_sample = X_train.sample(frac=1.0, replace=True, random_state=42)\n",
    "y_train_sample = y_train.loc[X_train_sample.index]\n",
    "\n",
    "# Build a \"weak\" Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=4, random_state=500)\n",
    "\n",
    "# Fit the model to the training sample\n",
    "clf.fit(X_train_sample, y_train_sample)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the list of individual models\n",
    "clf_list = []\n",
    "for i in range(21):\n",
    "\tweak_dt = build_decision_tree(X_train, y_train, random_state=i)\n",
    "\tclf_list.append(weak_dt)\n",
    "\n",
    "# Predict on the test set\n",
    "pred = predict_voting(clf_list, X_test)\n",
    "\n",
    "# Print the F1 score\n",
    "print('F1 score: {:.3f}'.format(f1_score(y_test, pred)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BaggingClassifier: nuts and bolts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the base model\n",
    "clf_dt = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Build the Bagging classifier\n",
    "clf_bag = BaggingClassifier(clf_dt, n_estimators=21, random_state=500)\n",
    "\n",
    "# Fit the Bagging model to the training set\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "pred = clf_bag.predict(X_test)\n",
    "\n",
    "# Show the F1-score\n",
    "print('F1-Score: {:.3f}'.format(f1_score(y_test, pred)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build and train the bagging classifier\n",
    "clf_bag = BaggingClassifier(\n",
    "  clf_dt,\n",
    "  n_estimators=21,\n",
    "  oob_score=True,\n",
    "  random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Print the out-of-bag score\n",
    "print('OOB-Score: {:.3f}'.format(clf_bag.oob_score_))\n",
    "\n",
    "# Evaluate the performance on the test set to compare\n",
    "# use the accuracy score so that you can easily compare it to the out-of-bag score.\n",
    "pred = clf_bag.predict(X_test)\n",
    "print('Accuracy: {:.3f}'.format(accuracy_score(y_test, pred)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bagging parameters: tips and tricks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# target is imbalanced, as more than 90% of the tests are negative. An individual model may be prone to overfitting, so it's a good idea to leverage an ensemble method here.\n",
    "\n",
    "# Build a balanced logistic regression\n",
    "# target has a high class imbalance, use a \"balanced\" logistic regression as the base estimator\n",
    "clf_lr = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
    "\n",
    "# Build and fit a bagging classifier\n",
    "clf_bag = BaggingClassifier(clf_lr, max_features=10, oob_score=True, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set and show the out-of-bag score\n",
    "pred = clf_bag.predict(X_test)\n",
    "print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, pred)))\n",
    "print('OOB-Score: {:.2f}'.format(clf_bag.oob_score_))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build a balanced logistic regression\n",
    "# target has a high class imbalance, use a \"balanced\" logistic regression as the base estimator\n",
    "clf_lr = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
    "\n",
    "# Build and fit a bagging classifier\n",
    "clf_bag = BaggingClassifier(clf_lr, max_features=10, oob_score=True, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set and show the out-of-bag score\n",
    "# out-of-bag score is a good indicator of the actual performance on unseen data\n",
    "pred = clf_bag.predict(X_test)\n",
    "print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, pred)))\n",
    "print('OOB-Score: {:.2f}'.format(clf_bag.oob_score_))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build a balanced logistic regression\n",
    "clf_base = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
    "\n",
    "# Build and fit a bagging classifier with custom parameters\n",
    "# sample without replacement.\n",
    "clf_bag = BaggingClassifier(clf_base, n_estimators=20, max_features=10, max_samples=0.65, bootstrap=False, random_state=500)\n",
    "clf_bag.fit(X_train, y_train)\n",
    "\n",
    "# Calculate predictions and evaluate the accuracy on the test set\n",
    "y_pred = clf_bag.predict(X_test)\n",
    "print('Accuracy:  {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
